hydra: 
  run:
    # Configure output dir of each experiment programmatically from the arguments
    dir: outputs/${model.data.name}${model.data.n_classes}/${model.name}/${experiment}/${mode}/${now:%d.%b.%y-%H:%M:%S.%f}

# Global configurations shared between different modules
experiment: default
profile: True
seed: 1234 # 0 stands for not fixing the seed
mode: fit # fit, validate, test

# Composing nested config with default
defaults:
  - _self_
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

# Pytorch lightning trainer's argument
# default flags are commented to avoid clustering the hyperparameters
trainer:
  # accelerator: None
  # accumulate_grad_batches: 1
  # amp_backend: native
  # amp_level: O2
  # auto_lr_find: False
  # auto_scale_batch_size: False
  # auto_select_gpus: False
  # benchmark: True
  check_val_every_n_epoch: 10
  # checkpoint_callback: True
  # default_root_dir:
  # deterministic: False
  # fast_dev_run: False
  # flush_logs_every_n_steps: 100
  gpus: [0]
  # gradient_clip_val: .5
  # gradient_clip_algorithm: "norm"
  # limit_predict_batches: 1.0
  # limit_test_batches: 1.0
  # limit_train_batches: 1.0
  # limit_val_batches: 1.0
  # log_every_n_steps: 50
  # log_gpu_memory: False
  # logger: True
  max_epochs: 10000
  # max_steps: None
  # min_epochs: None
  # min_steps: None
  # move_metrics_to_cpu: False
  # multiple_trainloader_mode: max_size_cycle
  # num_nodes: 1
  # num_processes: 1
  # num_sanity_val_steps: 2
  # overfit_batches: 0.0
  # plugins: None
  # precision: 16
  # prepare_data_per_node: True
  # process_position: 0
  # profiler: None
  # progress_bar_refresh_rate: None
  # reload_dataloaders_every_epoch: False
  # replace_sampler_ddp: True
  # resume_from_checkpoint: None
  # stochastic_weight_avg: False
  # sync_batchnorm: False
  # terminate_on_nan: False
  # track_grad_norm: -1
  # truncated_bptt_steps: None
  # val_check_interval: 1.0
  # weights_save_path: None
  # weights_summary: top